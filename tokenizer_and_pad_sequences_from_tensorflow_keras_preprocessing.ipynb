{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a training notebook to help me uderstand some **tensorflow.keras.preprocessing** classesand and methods:\n",
    "* Tokenizer --> [Tokenizer docs](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer)\n",
    "* pad_sequences (padding) --> [padding docs](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    'I love my dog',\n",
    "    'i love my cat',\n",
    "    'You love my dog!',\n",
    "    'Do you think my dog is amazing?'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = 100) # num_words = max words\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_index: \n",
      "{'love': 1, 'my': 2, 'dog': 3, 'i': 4, 'cat': 5, 'you': 6, 'is': 7, 'do': 8, 'think': 9, 'amazing': 10, 'your': 11}\n",
      "\n",
      "sequences: \n",
      "[[4, 1, 2, 3], [4, 1, 2, 5], [6, 1, 2, 3], [8, 6, 9, 2, 3, 7, 10], [3, 5, 7, 11, 1]]\n"
     ]
    }
   ],
   "source": [
    "print('word_index: \\n{}\\n'.format(word_index))\n",
    "print('sequences: \\n{}'.format(sequences)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [\n",
    "    'i really love my dog',\n",
    "    'my dog loves my manatee'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 1, 2, 3], [2, 3, 2]]\n"
     ]
    }
   ],
   "source": [
    "test_seq = tokenizer.texts_to_sequences(test_data)\n",
    "print(test_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "words that are not in tokenizer.word_index were ignored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenizer with out of vocabulary token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = 100, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use any word for out of index token eg. oov_token='blablabla' if you want to receive word_index: \n",
    "{'blablabla': 1, (...)} (:\n",
    "Just use unique one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_index: \n",
      "{'<OOV>': 1, 'love': 2, 'my': 3, 'dog': 4, 'i': 5, 'cat': 6, 'you': 7, 'is': 8, 'do': 9, 'think': 10, 'amazing': 11, 'your': 12}\n",
      "\n",
      "sequences: \n",
      "[[5, 2, 3, 4], [5, 2, 3, 6], [7, 2, 3, 4], [9, 7, 10, 3, 4, 8, 11], [4, 6, 8, 12, 2]]\n"
     ]
    }
   ],
   "source": [
    "print('word_index: \\n{}\\n'.format(word_index))\n",
    "print('sequences: \\n{}'.format(sequences))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [\n",
    "    'i really love my dog',\n",
    "    'my dog loves my manatee'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5, 1, 2, 3, 4], [3, 4, 1, 3, 1]]\n"
     ]
    }
   ],
   "source": [
    "test_seq = tokenizer.texts_to_sequences(test_data)\n",
    "print(test_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "words that are not in tokenizer.word_index were indexed as 1 (<-- the out of index token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the num_words param\n",
    "* num_words set as 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = 3)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_index: \n",
      "{'love': 1, 'my': 2, 'dog': 3, 'i': 4, 'cat': 5, 'you': 6, 'is': 7, 'do': 8, 'think': 9, 'amazing': 10, 'your': 11}\n",
      "\n",
      "sequences: \n",
      "[[1, 2], [1, 2], [1, 2], [2], [1]]\n"
     ]
    }
   ],
   "source": [
    "print('word_index: \\n{}\\n'.format(word_index))\n",
    "print('sequences: \\n{}'.format(sequences))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it does not have an impact on fit_on_texts outputs such as word_index, word_counts, word_docs (all words were used), but only on final sequences that were made (eg. texts_to_sequences or texts_to_matrix methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does a Tokenizer change a text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    'I \\n \\n love my \\n dog',\n",
    "    'i love my           cat...',\n",
    "    'He loves my dog!',\n",
    "    '   Do you think           my dog is amazing? ',\n",
    "    'dog-cat is your love',\n",
    "    'i found 1234 cats in my house',\n",
    "    'Is 12cat3445H2O or dog15-14 a good name for a cat? '\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = 100) # num_words = max words\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_index: \n",
      "{'my': 1, 'dog': 2, 'i': 3, 'love': 4, 'cat': 5, 'is': 6, 'a': 7, 'he': 8, 'loves': 9, 'do': 10, 'you': 11, 'think': 12, 'amazing': 13, 'your': 14, 'found': 15, '1234': 16, 'cats': 17, 'in': 18, 'house': 19, '12cat3445h2o': 20, 'or': 21, 'dog15': 22, '14': 23, 'good': 24, 'name': 25, 'for': 26}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('word_index: \\n{}\\n'.format(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some Tokenizer default values (can be defined when Tokenizer() is instantiated):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, ' ', False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.lower, tokenizer.split, tokenizer.char_level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tensorflow.keras.preprocessing.text.Tokenizer :\n",
    "* lowercase --> I = i, Do = do\n",
    "* ignore redundant whitespaces\n",
    "* replace punctation included in tokenizer.filters by whitespace --> 'dog15-14' was indexed as 2 words: 'dog15' and '14' so probably after removing punctation 'dog15 14' was the output\n",
    "* some special characters like newline \\n and tab \\t are also removed with punctation\n",
    "* numbers are not removed both: seperated numbers and in joins with words\n",
    "* you and your, cat and cats, love and loves are different words/indexes\n",
    "* no stop words included"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# padding\n",
    "* making the sentences the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    'I love my dog',\n",
    "    'i love my cat',\n",
    "    'You love my dog!',\n",
    "    'Do you think my dog is amazing?'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  no changes :)\n",
    "tokenizer = Tokenizer(num_words = 100, oov_token='<OOV>') \n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "sequences = tokenizer.texts_to_sequences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  padding\n",
    "padded = pad_sequences(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_index: \n",
      "{'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n",
      "\n",
      "sequences: \n",
      "[[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n",
      "\n",
      "padded: \n",
      "[[ 0  0  0  5  3  2  4]\n",
      " [ 0  0  0  5  3  2  7]\n",
      " [ 0  0  0  6  3  2  4]\n",
      " [ 8  6  9  2  4 10 11]]\n"
     ]
    }
   ],
   "source": [
    "print('word_index: \\n{}\\n'.format(word_index))\n",
    "print('sequences: \\n{}\\n'.format(sequences))\n",
    "print('padded: \\n{}'.format(padded)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* so the default is padding='pre' (at the beggining)\n",
    "* this is probably also the answer, why index 0 is not used in word_index (:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### padding = 'post'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padded: \n",
      "[[ 5  3  2  4  0  0  0]\n",
      " [ 5  3  2  7  0  0  0]\n",
      " [ 6  3  2  4  0  0  0]\n",
      " [ 8  6  9  2  4 10 11]]\n"
     ]
    }
   ],
   "source": [
    "padded = pad_sequences(sequences, padding = 'post')\n",
    "print('padded: \\n{}'.format(padded)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### maxlen param\n",
    "* default cut at the front"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padded: \n",
      "[[ 5  3  2  4  0]\n",
      " [ 5  3  2  7  0]\n",
      " [ 6  3  2  4  0]\n",
      " [ 9  2  4 10 11]]\n"
     ]
    }
   ],
   "source": [
    "padded = pad_sequences(sequences, padding = 'post', maxlen=5)\n",
    "print('padded: \\n{}'.format(padded)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### maxlen param\n",
    "* truncating = 'post' --> cutting at the back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padded: \n",
      "[[5 3 2 4 0]\n",
      " [5 3 2 7 0]\n",
      " [6 3 2 4 0]\n",
      " [8 6 9 2 4]]\n"
     ]
    }
   ],
   "source": [
    "padded = pad_sequences(sequences, padding = 'post', \n",
    "                       truncating = 'post', maxlen=5)\n",
    "print('padded: \\n{}'.format(padded)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "just curiosity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
